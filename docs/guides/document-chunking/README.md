# ðŸ§  Insight IngeniousÂ â€“ DocumentÂ Chunking

A robust, extensible service **and** CLI for splitting documents into precise, contextâ€‘aware chunksâ€”perfect for Retrievalâ€‘Augmented Generation (RAG) pipelines.

---

## Overview

The **`chunk`** module provides the core textâ€‘splitting capabilities within the Insightâ€¯Ingenious framework. It transforms large source documents (Text, Markdown, JSON, JSONL) into smaller segments that balance semantic coherence with Large Language Model token limits.

**Ideal for:**

* Preparing data for vector databases and embedding models.
* Consistent, tokenâ€‘aware splitting across mixed document types.
* Bidirectional context overlap between chunks to boost retrieval quality.

---

## Features

* âœ¨ **Multiple Strategies**Â â€“ recursive, markdown, token, and semantic splitting.
* ðŸ“ **Precise Budgeting**Â â€“ configure `chunk_size` & `chunk_overlap` in tokens (via *tiktoken*) **or** characters.
* ðŸ”— **Bidirectional Overlap**Â â€“ overlap before *and* after each chunk for maximum context preservation.
* ðŸŒ **Unicode Safe**Â â€“ token strategy respects grapheme boundaries, protecting complex characters & emojis.
* ðŸ§  **Semantic Splitting**Â â€“ OpenAIâ€¯/â€¯Azureâ€¯OpenAI embeddings find natural semantic breaks.
* âš¡ **Efficient Loading**Â â€“ streams large JSON via *ijson* to minimise memory.
* ðŸ†” **Stable IDs**Â â€“ deterministic, globallyâ€‘unique chunk IDs with configurable path encoding.

---

## Installation

The chunking capabilities are an **optional extra**.

```bash
# Install the core Ingenious package with the chunking extraâ€”assuming youâ€™ve already run pip install uv, created your environment with uv venv, and activated it.
uv pip install -e ".[chunk]"
```

> **Note**Â Â Semantic splitting requires access to OpenAI or Azure OpenAI embeddings.

---

## QuickÂ StartÂ (CLI)

The primary entryâ€‘point is `ingen chunk run`.

### BasicÂ RecursiveÂ Splitting

Split a text file into 100â€‘token chunks with a 10â€‘token overlap:

```bash
ingen chunk run test_files/git_repo_docs.txt --strategy token --overlap-unit tokens --chunk-size 100 --chunk-overlap 10 --encoding-name cl100k_base --output test_integration_chunking_folder_2/git_repo_docs-token-tokens.jsonl
```

### ProcessingÂ JSONLÂ Input

Chunk each record from a JSONâ€¯Lines file (e.g. output from *ingen documentâ€‘processing*):

```bash
ingen chunk run test_files/pages_pdfminer_local_patientbrochure.jsonl --strategy token --overlap-unit tokens --chunk-size 100 --chunk-overlap 10 --encoding-name cl100k_base --output test_integration_chunking_folder_2/pages_pdfminer_local_patientbrochure-token-tokens.jsonl
```

### Semantic Splitting the Azure variables below or, alternatively, *(requires `OPENAI_API_KEY`)*

#### Azure OpenAI Service

```bash
export AZURE_OPENAI_API_KEY="ae7a5e4566yheayse5y754223ryaergarg"
export AZURE_OPENAI_ENDPOINT="https://testsemanticchunking.openai.azure.com/"
export AZURE_EMBEDDING_DEPLOYMENT="text-embedding-3-small"
```

#### Standard OpenAI Key (alternative)

```bash
export OPENAI_API_KEY="your-openai-api-key"
```

Example command:

```bash
ingen chunk run test_files/git_repo_docs.txt \
  --strategy semantic \
  --overlap-unit tokens \
  --chunk-size 100 \
  --chunk-overlap 10 \
  --encoding-name cl100k_base \
  --output test_integration_chunking_folder_2/git_repo_docs-semantic-tokens.jsonl
```
---

## PythonÂ API

```python
from ingenious.chunk import ChunkConfig, build_splitter

# 1 Â· Define configuration
config = ChunkConfig(
    strategy="token",
    chunk_size=256,
    chunk_overlap=32,
    overlap_unit="tokens"
)

# 2 Â· Build the splitter instance
splitter = build_splitter(config)

# 3 Â· Split text
text = "Your long document content goes here..."
chunks = splitter.split_text(text)

print(f"Generated {len(chunks)} chunks.")
# print(chunks[0])
```

---

## ConfigurationÂ &Â Strategies

Behaviour is controlled via CLI flags that map 1â€‘toâ€‘1 with the `ChunkConfig` model.

### AvailableÂ Strategies

| Strategy    | Description                                                        | KeyÂ Configuration                                                        |
| ----------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------ |
| `recursive` | Hierarchical splits (paragraphâ†’sentenceâ†’word). Fast and versatile. | `--chunk-size`, `--chunk-overlap`, `--separators`                        |
| `markdown`  | Aware of Markdown structure (headers, lists).                      | `--chunk-size`, `--chunk-overlap`                                        |
| `token`     | Splits on token boundaries, Unicodeâ€‘safe.                          | `--chunk-size`, `--chunk-overlap`, `--encoding-name`                     |
| `semantic`  | Uses embeddings to split at semantic breaks.                       | `--embed-model`, `--azure-deployment`, `--semantic-threshold-percentile` |

### CoreÂ Options

| Flag              | Description                                      | Default       |
| ----------------- | ------------------------------------------------ | ------------- |
| `--strategy`      | Splitting algorithm.                             | `recursive`   |
| `--chunk-size`    | Max size of each chunk (tokens/chars).           | `1024`        |
| `--chunk-overlap` | Overlap between adjacent chunks.                 | `128`         |
| `--overlap-unit`  | Unit for size/overlap: `tokens` or `characters`. | `tokens`      |
| `--encoding-name` | *tiktoken* encoding for token counting.          | `cl100k_base` |

---

## InputÂ FileÂ Contract

| Format                     | Handling                                                                         |
| -------------------------- | -------------------------------------------------------------------------------- |
| `.txt`, `.md`, `.markdown` | Entire file treated as one document.                                             |
| `.json`                    | Object or array; each object must include `text`, `page_content`, **or** `body`. |
| `.jsonl`, `.ndjson`        | One JSON object per line with the keys above.                                    |

---

## AdvancedÂ Â·Â Stable ChunkÂ IDs

CLI generates deterministic IDs: `<prefix>#p<page>.<pos>-<hash>` where `<prefix>` is set by `--id-path-mode`.

* **`rel`** *(default)*Â â€“ path relative to CWD (or `--id-base`). Falls back to hashed absâ€‘path when outside base.
* **`hash`**Â â€“ always truncated SHAâ€‘256 of absâ€‘path. Good for privacy / crossâ€‘machine stability.
* **`abs`**Â â€“ absolute file system path (may leak info). Requires `--force-abs-path`.

```bash
# Example: use hashing for the ID prefix
ingen chunk run my_document.txt --id-path-mode hash
```

---

## DevelopmentÂ &Â Testing

```bash
# Install testing dependencies
uv pip install -e ".[chunk,test]"

# Run the test suite
uv run pytest ingenious/chunk/tests
```
